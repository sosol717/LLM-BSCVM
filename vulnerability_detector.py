import os
from typing import Dict, Any, List, Set, Tuple, Optional
import torch
import logging
import json
import numpy as np
import re
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from config.config import Config
import random
import torch
import numpy as np


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
class VulnerabilityConfig:
    """漏洞检测配置类"""
    # 风险等级及对应分数
    RISK_LEVELS = {
        'Critical': 1.0,
        'High': 0.8,
        'Medium': 0.5,
        'Low': 0.2
    }
    
    # 基础权重配置
    BASE_WEIGHTS = {
        'model': 0.4,
        'static': 0.3,
        'similar': 0.4
    }

    # 漏洞模式配置
    VULNERABILITY_PATTERNS = {
        'reentrancy': {
            'weight': 0.3,
            'risk_level': 'Critical',
            'patterns': [
                (r'(\.\s*call\s*\{.*?\})', 0.4),
                (r'(external\s+function)', 0.2),
                (r'(\.transfer\()', 0.2),
                (r'(\.send\()', 0.2)
            ]
        },
        'access_control': {
            'weight': 0.25,
            'risk_level': 'Critical',
            'patterns': [
                (r'(onlyOwner)', 0.3),
                (r'(require\s*\([^)]*msg\.sender[^)]*\))', 0.3),
                (r'(modifier\s+\w+)', 0.2),
                (r'(tx\.origin)', 0.2)
            ]
        },
        'arithmetic': {
            'weight': 0.2,
            'risk_level': 'High',
            'patterns': [
                (r'(\+\+|\+=)', 0.3),
                (r'(\-\-|\-=)', 0.3),
                (r'(\*=|\/=)', 0.2),
                (r'(?<!SafeMath\.)(\+|-|\*|\/)', 0.2)
            ]
        },
        'unchecked_calls': {
            'weight': 0.15,
            'risk_level': 'Critical',
            'patterns': [
                (r'(\.call\{)', 0.4),
                (r'(\.delegatecall)', 0.3),
                (r'(assembly\s*\{)', 0.2),
                (r'(selfdestruct|suicide)', 0.1)
            ]
        },
        'timestamp_dependency': {
            'weight': 0.1,
            'risk_level': 'Medium',
            'patterns': [
                (r'block\.timestamp', 0.4),
                (r'now', 0.3),
                (r'block\.number', 0.3)
            ]
        }
    }

class VulnerabilityDetector(BaseAnalyzer):
    """增强版智能合约漏洞检测器"""
    
    def __init__(self, model_path: str = None, knowledge_base_path: str = None, ablation_mode: str = None):
        """
        初始化检测器
        Args:
            model_path: 预训练模型路径
            knowledge_base_path: 知识库路径
        """
        # 设置随机数种子为42
        random.seed(1)
        np.random.seed(1)
        torch.manual_seed(1)
        torch.cuda.manual_seed_all(1)  # 如果使用CUDA

        self.ablation_mode = ablation_mode
        super().__init__()
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        try:
            # 1. 确定路径
            self.model_path = model_path if model_path else Config.FINETUNED_MODEL_PATH
            self.kb_path = knowledge_base_path if knowledge_base_path else Config.KNOWLEDGE_BASE_PATH
            
            logger.info(f"Using model path: {self.model_path}")
            logger.info(f"Using knowledge base path: {self.kb_path}")
            
            # 2. 加载模型组件
            self.model = self._load_model(self.model_path)
            self.tokenizer = self._load_tokenizer(self.model_path)
            
            # 3. 加载知识库组件
            self.knowledge_base = self._load_knowledge_base(self.kb_path)
            self.vectorizer = self._init_vectorizer()
            
            # 4. 初始化漏洞模式库
            self._init_vulnerability_patterns()
            
            logger.info("Successfully initialized vulnerability detector")
            
        except Exception as e:
            logger.error(f"Error initializing detector: {str(e)}")
            raise

    def _init_vulnerability_patterns(self):
        """初始化漏洞模式库"""
        self.vulnerability_patterns = VulnerabilityConfig.VULNERABILITY_PATTERNS

    def _get_risk_level_score(self, risk_level: str) -> float:
        """根据风险等级返回分数"""
        return VulnerabilityConfig.RISK_LEVELS.get(risk_level, 0.0)

    def _load_model(self, model_path: str):
        """加载模型"""
        try:
            logger.info(f"Loading model from {model_path}")
            # 确保环境中没有代理设置
            for env_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
                if env_var in os.environ:
                    del os.environ[env_var]
                    
            device_id = Config.DEVICE_MAP.get('vulnerability_detector', 0)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_path,
                num_labels=2,
                torch_dtype=torch.float16,
                device_map={'': device_id},
                use_auth_token=False,  # 不使用认证token
                local_files_only=True  # 优先使用本地文件
            )
            return model
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            raise

    def _load_tokenizer(self, model_path: str):
        """加载tokenizer"""
        try:
            # 确保环境中没有代理设置
            for env_var in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
                if env_var in os.environ:
                    del os.environ[env_var]
                    
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                use_auth_token=False,  # 不使用认证token
                local_files_only=True  # 优先使用本地文件
            )
            return tokenizer
        except Exception as e:
            logger.error(f"Error loading tokenizer: {str(e)}")
            raise

    def _load_knowledge_base(self, file_path: str) -> List[Dict]:
        """加载知识库"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading knowledge base: {str(e)}")
            return []


    def _generate_prompt(self, code: str) -> str:
        """生成模型输入的prompt"""
        return f"""[INST] <<SYS>> You are a smart contract security auditor. Analyze the given Solidity code for potential vulnerabilities. Return ONLY "Safe" or "Vulnerable". <</SYS>>

Analyze this smart contract code for security vulnerabilities:

```solidity
{code}

Determine if this contract is safe or vulnerable. [/INST]"""   

    def analyze(self, contract_code: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """分析合约代码"""
        try:
            # 1. 获取模型预测
            model_prediction = self._get_model_prediction(contract_code)
            
            # 2. 执行静态分析
            static_score, static_findings = self._get_static_score(contract_code)
            
            # 3. 进行相似度分析
            similar_score, similar_contracts = self._get_similar_score(contract_code)
            
            # 4. 整合所有分析结果
            result = self._combine_scores(
                model_prediction,
                (static_score, static_findings),
                (similar_score, similar_contracts)
            )
            
            logger.info(f"Final prediction: {result['vulnerability_check']}")
            logger.info(f"Confidence: {result['confidence_score']:.4f}")
            
            return result

        except Exception as e:
            logger.error(f"Error in vulnerability analysis: {str(e)}")
            return {
                "vulnerability_check": "Vulnerable",  # 错误时默认为可能存在漏洞
                "confidence_score": 0.0,
                "error": str(e)
            }
    

    def _get_model_prediction(self, code: str) -> Tuple[str, float, Dict[str, float]]:
        """获取模型预测结果"""
        prompt = self._generate_prompt(code)
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.nn.functional.softmax(logits, dim=1)
            prediction = torch.argmax(logits, dim=1).item()
            confidence = probs[0][prediction].item()

        prediction_label = "Safe" if prediction == 0 else "Vulnerable"
        probabilities = {
            "safe": probs[0][0].item(),
            "vulnerable": probs[0][1].item()
        }
        
        return prediction_label, confidence, probabilities

    def _get_static_score(self, code: str) -> Tuple[float, Dict]:
        """计算静态分析分数"""
        findings = {}
        total_score = 0.0
        
        for vuln_type, config in self.vulnerability_patterns.items():
            type_score = 0.0
            matches = []
            
            for pattern, weight in config['patterns']:
                for match in re.finditer(pattern, code):
                    line_number = code.count('\n', 0, match.start()) + 1
                    context = code[max(0, match.start()-50):min(len(code), match.start()+50)]
                    matches.append({
                        'line': line_number,
                        'match': match.group(),
                        'context': context.strip(),
                        'weight': weight
                    })
                    type_score += weight
            
            if matches:
                findings[vuln_type] = matches
                # 归一化该类型的分数并应用类型权重
                normalized_score = min(1.0, type_score)
                total_score += normalized_score * config['weight']

        return total_score, findings

    def _get_similar_score(self, code: str) -> Tuple[float, List[Dict]]:
        """计算相似度分数"""
        if not self.knowledge_base or not hasattr(self, 'kb_vectors'):
            # 首次运行时，处理知识库
            kb_texts = [f"{entry.get('code', '')} {entry.get('description', '')}" 
                    for entry in self.knowledge_base]
            if not kb_texts:
                return 0.0, []
                
            self.vectorizer.fit(kb_texts)
            self.kb_vectors = self.vectorizer.transform(kb_texts)
        
        try:
            query_vector = self.vectorizer.transform([code])
            similarities = cosine_similarity(query_vector, self.kb_vectors).flatten()
            
            # 获取top-k相似合约
            k = min(5, len(similarities))
            top_indices = similarities.argsort()[-k:][::-1]
            similar_contracts = []
            
            weighted_score = 0.0
            total_weight = sum(1/(i+1) for i in range(k))
            
            for i, idx in enumerate(top_indices):
                weight = 1/(i+1)/total_weight
                contract = self.knowledge_base[idx]
                similarity = similarities[idx]
                
                contract_info = {
                    'similarity': similarity,
                    'label': contract.get('ground_truth_label', 'unknown'),
                    'code': contract.get('code', '')[:200]  # 只保留前200个字符
                }
                similar_contracts.append(contract_info)
                
                is_vulnerable = contract.get('ground_truth_label', '').lower() == 'vulnerable'
                weighted_score += weight * (1.0 if is_vulnerable else 0.0)
            
            return weighted_score, similar_contracts
                
        except Exception as e:
            logger.error(f"Error in similarity analysis: {str(e)}")
            return 0.0, []
        
        
    def _combine_scores(self, 
                       model_pred: Tuple[str, float, Dict[str, float]], 
                       static_result: Tuple[float, Dict],
                       similar_result: Tuple[float, List[Dict]]) -> Dict[str, Any]:
        """增强的分数组合方法"""
        model_label, model_conf, model_probs = model_pred
        static_score, static_findings = static_result
        similar_score, similar_contracts = similar_result
        
        # 1. 基于模型置信度的分层决策
        if model_conf > 0.85:
            # 模型高度确信时，直接采用模型结果
            final_prediction = model_label
            combined_score = model_probs['vulnerable' if model_label == 'Vulnerable' else 'safe']
            
        elif model_conf > 0.6:
            # 模型比较确信时，轻度结合其他分析
            weights = VulnerabilityConfig.BASE_WEIGHTS
            combined_score = self._calculate_weighted_score(
                model_probs['vulnerable'], static_score, similar_score, weights)
            final_prediction = "Vulnerable" if combined_score > 0.5 else "Safe"
            
        else:
            # 模型不够确信时，深度结合其他分析
            static_confidence = self._calculate_static_confidence(static_findings)
            similar_confidence = self._calculate_similar_confidence(similar_contracts)
            
            weights = self._get_dynamic_weights(
                model_conf, static_confidence, similar_confidence)
            
            combined_score = self._calculate_weighted_score(
                model_probs['vulnerable'], static_score, similar_score, weights)
            
            threshold = self._calculate_dynamic_threshold(
                model_conf, static_confidence, similar_confidence)
            
            final_prediction = "Vulnerable" if combined_score > threshold else "Safe"
        
        # 获取高风险的漏洞类型
        high_risk_findings = self._get_high_risk_findings(static_findings)
        
        return {
            "vulnerability_check": final_prediction,
            "confidence_score": combined_score,
            "analysis_details": {
                "model_analysis": {
                    "prediction": model_label,
                    "confidence": model_conf,
                    "probabilities": model_probs
                },
                "static_analysis": {
                    "score": static_score,
                    "findings": static_findings,
                    "high_risk_findings": high_risk_findings
                },
                "similarity_analysis": {
                    "score": similar_score,
                    "similar_contracts": similar_contracts[:3]
                }
            }
        }
        

    def _calculate_weighted_score(self,
                                model_score: float,
                                static_score: float,
                                similar_score: float,
                                weights: Dict[str, float]) -> float:
        """计算加权分数"""
        return (
            weights['model'] * model_score +
            weights['static'] * static_score +
            weights['similar'] * similar_score
        )

    def _get_high_risk_findings(self, findings: Dict) -> List[Dict]:
        """获取高风险漏洞发现"""
        high_risk_findings = []
        for vuln_type, matches in findings.items():
            if vuln_type in self.vulnerability_patterns:
                pattern_info = self.vulnerability_patterns[vuln_type]
                risk_level = pattern_info.get('risk_level', 'Low')
                if risk_level in ['Critical', 'High']:
                    high_risk_findings.extend([
                        {
                            'type': vuln_type,
                            'risk_level': risk_level,
                            'line': match['line'],
                            'context': match['context']
                        }
                        for match in matches
                    ])
        return sorted(high_risk_findings, 
                     key=lambda x: self._get_risk_level_score(x['risk_level']),
                     reverse=True)

    def _get_dynamic_weights(self, 
                           model_conf: float, 
                           static_conf: float, 
                           similar_conf: float) -> Dict[str, float]:
        """动态计算各组件权重"""
        # 使用配置的基础权重
        weights = VulnerabilityConfig.BASE_WEIGHTS.copy()
        
        # 基于置信度调整权重
        total_conf = model_conf + static_conf + similar_conf
        if total_conf > 0:
            weights['model'] = model_conf / total_conf * 0.4 + 0.6 # 保证模型最低权重0.3
            weights['static'] = static_conf / total_conf * 0.2
            weights['similar'] = similar_conf / total_conf * 0.2
            
        # 归一化
        total = sum(weights.values())
        return {k: v/total for k, v in weights.items()}

    def _calculate_static_confidence(self, findings: Dict) -> float:
        """计算静态分析的置信度"""
        if not findings:
            return 0.0
            
        # 计算每种漏洞类型的置信度
        type_confidences = []
        for vuln_type, matches in findings.items():
            # 获取该类型的配置
            type_config = self.vulnerability_patterns.get(vuln_type, {})
            type_weight = type_config.get('weight', 0.0)
            
            # 计算该类型的置信度
            pattern_scores = []
            for match in matches:
                context_score = self._evaluate_context_safety(match['context'])
                pattern_scores.append(match['weight'] * context_score)
            
            if pattern_scores:
                type_confidences.append(max(pattern_scores) * type_weight)
        
        return sum(type_confidences) / len(type_confidences) if type_confidences else 0.0

    def _calculate_similar_confidence(self, similar_contracts: List[Dict]) -> float:
        """计算相似度分析的置信度"""
        if not similar_contracts:
            return 0.0
            
        # 计算基于相似度的置信度
        confidences = []
        total_weight = sum(1/(i+1) for i in range(len(similar_contracts)))
        
        for i, contract in enumerate(similar_contracts):
            weight = 1/(i+1)/total_weight
            similarity = contract.get('similarity', 0.0)
            # 相似度越高，置信度越高
            confidences.append(weight * similarity)
            
        return sum(confidences)

    def _calculate_dynamic_threshold(self,
                                  model_conf: float,
                                  static_conf: float,
                                  similar_conf: float) -> float:
        """计算动态阈值"""
        # 基础阈值
        base_threshold = 0.6
        
        # 根据置信度调整阈值
        if model_conf > 0.85:
            # 模型高置信时降低阈值
            threshold_adjustment = -0.05
        elif model_conf < 0.6:
            # 模型低置信时提高阈值
            threshold_adjustment = 0.05
        else:
            threshold_adjustment = 0
            
        # 考虑静态分析和相似度分析的影响
        if static_conf > 0.8 or similar_conf > 0.8:
            threshold_adjustment -= 0.02
            
        return max(0.4, min(0.6, base_threshold + threshold_adjustment))

    def _evaluate_context_safety(self, context: str) -> float:
        """评估代码上下文的安全性"""
        safety_patterns = [
            (r'require\s*\([^)]*\)', 0.2),  # 使用require进行检查
            (r'assert\s*\([^)]*\)', 0.15),   # 使用assert进行检查
            (r'SafeMath', 0.15),             # 使用SafeMath库
            (r'revert\s*\([^)]*\)', 0.1),    # 使用revert进行回滚
            (r'onlyOwner', 0.1),             # 使用访问控制修饰符
            (r'modifier', 0.1),              # 使用自定义修饰符
        ]
        
        context_score = 1.0
        for pattern, weight in safety_patterns:
            if re.search(pattern, context):
                context_score -= weight
                
        return max(0.2, context_score)  # 保证最低分数为0.2
    
    def __del__(self):
        """清理资源"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        